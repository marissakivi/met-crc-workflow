####################
# Modeling Met Ensemble Workflow
# Step 8 :: Generation of ensemble weights 
####################
# This script has been modified so it can be run as a job submission to the CRC's machines. 
#   
# Description: This step contains a modified version of John Tipton's filtering script. 
# The script works with many different data sources, in addition to the met generated by the 
# first parts of the workflow in order to weight the ensembles based on accuracy in 
# representing both PDSI and air temperature. These data sources are listed below. 

# Temparture Data Sources: 
# reconstruction - MANN data (500-2006)
# calibration - PRISM data (1895-2017)

# PDSI Data Sources: 
# reconstruction - LBDA data (1135-2005)
# calibration - ESRL data (1850-2014)
#
# Required functions: 
# - multiplot.R
#
# Required libraries:
# - sp
# - raster
# - RNetCDF
# - maps
# - stringr
# - lubridate
# - latex2exp
# - limSolve
# - ggplot2

####################
# ALTER ONLY THESE VARIABLES BEFORE SUBMITTING FOR NEW SITE
####################

# Load site and directory details
wd.base = '~/met'
site = "NRP" # should be identical to paleon site name 
site.name = 'North Round Pond' # for graph titling purposes
site.lat  = 42.84514
site.lon  = -72.447
vers=".v1"

# input years the met ensembles were generated for (long or short run?)
ens.yr1 = 850
ens.yr2 = 2015

# this should be true if submitting through the CRC so the program is not making unnecessary plots
CRC = TRUE

####################
# Step 1: Set up working directory
####################

# PRISM dates for anomaly adjustment (based on Mann paper?)
prism.yr1 = 1960
prism.yr2 = 1990

# years for calculation of squared error (?? 1949-1990 right now)
error.yr1 = 1949
error.yr2 = 1990

# year to plot for ensemble weight figures
plot = TRUE 
tt = 1750 

# number of filter draws 
K = 100 

# load required libraries 
if (!require('sp',lib='~/Rlibs')) install.packages('sp',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('raster',lib='~/Rlibs')) install.packages('raster',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('ncdf4',lib='~/Rlibs')) install.packages('RNetCDF',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('maps',lib='~/Rlibs')) install.packages('maps',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('stringr',lib='~/Rlibs')) install.packages('stringr',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('lubridate',lib='~/Rlibs')) install.packages('lubridate',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('latex2exp',lib='~/Rlibs')) install.packages('latex2exp',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('limSolve',lib='~/Rlibs')) install.packages('limSolve',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
if (!require('ggplot2',lib='~/Rlibs')) install.packages('ggplot2',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)

require(sp,lib='~/Rlibs')
require(raster,lib='~/Rlibs')
require(ncdf4,lib='~/Rlibs')
require(maps,lib='~/Rlibs')
require(stringr, lib='~/Rlibs')
require(lubridate,lib='~/Rlibs')
require(latex2exp,lib='~/Rlibs')
require(limSolve,lib='~/Rlibs')
require(ggplot2,lib='~/Rlibs')

sites <- c('HARVARD', 'TENSIONZONE', 'HEMLOCK', 'BIGWOODS', 'SYLVANIA', 
           'HURON', 'MORGAN', 'GOOSE', 'ROOSTER', 'GLSP', 'GILL', 'PALMGHATT',
           'CORAL', 'BONANZA', 'GLACIAL', 'PVC', 'NRP')
latlon  <- readLines(file.path(wd.base,"data/weight/PRISM","PRISM_ppt_temp_README.txt"))[50:66]
latlon = latlon[which(latlon == grep(site,latlon,value=TRUE))]
pl = str_locate(latlon[1],pattern=fixed("("))[1,1][[1]] + 1
long_site <- as.numeric(substr(latlon, pl, (pl+11)))
lat_site <- as.numeric(substr(latlon, (pl+13), (pl+24)))

path.func = file.path(wd.base,'functions')
# source required functions
source(file.path(path.func,"multiplot.R"))

# create save directory 
if (!dir.exists(file.path(wd.base,"ensembles",paste0(site,vers),"linkages","weights"))) {
  dir.create(file.path(wd.base,"ensembles",paste0(site,vers),"linkages","weights"),recursive=T)
}

####################
# Step 2: Load and prepare data for filtering
####################

## Load air temperature ensemble data
## final is temp.ens matrix with rows as years and columns as ensemble members
tair <- read.csv(file.path(wd.base,"ensembles",paste0(site,vers),'aggregated/month',"Temperature_AllMembers.csv"), 
                 stringsAsFactors=FALSE, header=TRUE)

# find the number of ensembles and remove the dates 
n_models <- ncol(tair) - 1
years <- ens.yr1:ens.yr2
models <- colnames(tair)[-1]

# convert from monthly means to annual means
temp.ens <- matrix(0, length(years), n_models)
for (i in 1:n_models) {
  temp.ens[, i] <- colMeans(matrix(tair[, i+1], nrow=12))
}

# name columns and rows with ensemble name and year 
rownames(temp.ens) <- years
colnames(temp.ens) <- colnames(tair)[-1]

## Load PDSI ensemble data 
## final is in pdsi.ens matrix with rows as years and columns as ensemble members
pdsi <- read.csv(file.path(wd.base,"ensembles",paste0(site,vers),'aggregated/month',"PDSI_AllMembers.csv"), 
                 stringsAsFactors=FALSE, header=TRUE)

# convert from monthly means to annual means
pdsi.ens <- matrix(0, length(years), n_models)
for (i in 1:n_models) {
  
  # use June, July, and August pdsi for annual summary
  pdsi.ens[, i] <- colMeans(matrix(pdsi[, i+1], nrow=12)[6:8, ])
  # pdsi.ens[, i] <- colMeans(matrix(pdsi[, i+1], nrow=12))
}

# name columns and rows with ensemble names and years 
rownames(pdsi.ens) <- years
colnames(pdsi.ens) <- colnames(pdsi)[-1]

# plot ensemble data for annual mean temperature and PDSI
if (!CRC){
  layout(matrix(1:2, 2, 1))
  matplot(years,temp.ens, type='l', col=adjustcolor(1:n_models, alpha.f=0.5),xlab='year',ylab='air temp',main='Ensemble Met Data')
  matplot(years,pdsi.ens, type='l', col=adjustcolor(1:n_models, alpha.f=0.5),xlab='year',ylab='pdsi')
}

## Prepare MANN data 
## Final is stored in temp.rec data frame with year, anomaly, and adjusted values 
mann.dat <- ncdf4::nc_open(file.path(wd.base,"data/weight","MANN.data.nc"))
m.lon = ncdf4::ncvar_get(mann.dat,'lon')
m.lat = ncdf4::ncvar_get(mann.dat,'lat')
m.tmpanm = ncdf4::ncvar_get(mann.dat,'tmp_anm')
m.tmp = ncdf4::ncvar_get(mann.dat,'tmp')
m.time = ncdf4::ncvar_get(mann.dat,'time')

# try
#mann.dat <- read.nc(mann)

# find closest point in MANN data to site coordinates
lon_mann <- which((m.lon - long_site)^2 == min((m.lon - long_site)^2))
lat_mann <- which((m.lat - lat_site)^2 == min((m.lat - lat_site)^2))

# plot site location on map
if (!CRC){
  layout(matrix(1:2, 2, 1))
  {image(m.lon, m.lat, m.tmp[, , 1300], 
         main=paste("spatial reconstruction -", site.name, "given as the green dot"))
    map(add=TRUE)
    points(m.lon[lon_mann], m.lat[lat_mann], pch=16, col="green") 
    
    # plot reconstructed temperature (MANN data) at site over time
    plot(m.time + 1901, m.tmpanm[lon_mann, lat_mann, ], 
         type='l', main=paste("reconstructed temperature at",site.name),
         ylab="Temperature", xlab="year")
    lines(m.time + 1901, temp.rec$temp, col="red",
          type='l')}
}

# save site temperature values
temp.rec = data.frame(
  temp = m.tmp[lon_mann, lat_mann, ],
  year = m.time+1901
)

## Prepare LBDA data
## Final is stored in pdsi.rec data frame with year and value
lbda.dat <- ncdf4::nc_open(file.path(wd.base,"data/weight","LBDA.data.nc")) ## try
l.lon = ncdf4::ncvar_get(lbda.dat,'lon')
l.lat = ncdf4::ncvar_get(lbda.dat,'lat')
l.time = ncdf4::ncvar_get(lbda.dat,'time')
l.pdsi = ncdf4::ncvar_get(lbda.dat,'pdsi')

# find LBDA data closest to site coordinates
lon_lbda <- which((l.lon - long_site)^2 == min((l.lon - long_site)^2))
lat_lbda <- which((l.lat - lat_site)^2 == min((l.lat - lat_site)^2))

# plot location on map
if (!CRC){
  layout(matrix(1:2, 2, 1))
  {image(l.lon, l.lat, t(l.pdsi[900, , ]), main=paste("PDSI data -",site.name,'given as green dot'),xlab='longitude',ylab='latitude')
    map(add=TRUE)
    points(l.lon[lon_lbda], l.lat[lat_lbda], pch=16, col="green")
    
    # plot reconstructed pdsi at site
    plot(l.time, l.pdsi[, lat_lbda, lon_lbda], 
         type='l', main=paste("reconstructed PDSI at", site.name), 
         ylab="PDSI", xlab="year")}
}

# find years with PDSI data at site 
pdsi.yrs <- which(l.pdsi[,lat_lbda,lon_lbda] != -99.999)
n_pdsi <- length(pdsi.yrs)

# save site PDSI
pdsi.rec = data.frame(
  pdsi = l.pdsi[pdsi.yrs, lat_lbda, lon_lbda],
  year = l.time[pdsi.yrs]
)

## Load and prepare PRISM data 
## Final is stored in temp.cal data frame with year and value 
load(file.path(wd.base,'data/weight/PRISM/paleon_sites',paste0(site,'.meanTemp.Rdata')))
temp.cal = meanTemp
year <- substr(rownames(temp.cal), 1, 4)

# calculate annual average temperature, ignoring missing values
temp.cal <- data.frame(
  temp = c(by(apply(temp.cal, 1, mean), year, mean)),
  year = as.numeric(as.character(unique(year))))

# convert to degrees F
temp.cal$temp <- temp.cal$temp * 1.8 + 32

# adjust temperature calibration dataset, correcting for anomaly using PRISM, save in temp.rec data frame
sd.prism = sd(subset(temp.cal$temp, temp.cal$year > prism.yr1 & temp.cal$year < prism.yr2))
mean.prism = mean(subset(temp.cal$temp, temp.cal$year > prism.yr1 & temp.cal$year < prism.yr2))
temp.rec$adj = temp.rec$temp*sd.prism + mean.prism

# plot calibration temperature dataset 
if (!CRC){
  layout(matrix(1:2, 2, 1))
  {plot(temp.cal$year, temp.cal$temp, type='l', ylab="temp", xlab="year", 
        main="calibration temperature dataset")
    lines(temp.rec$year, temp.rec$adj, col='red',type='l')
    
    # plot temperature anomaly for error years
    plot(subset(temp.rec$year, temp.rec$year > error.yr1),
         subset(temp.rec$temp, temp.rec$year > error.yr1),
         col="red", type='l', ylab="temperature anomaly", xlab="year",
         main="Mann reconstructed temperature anomaly")}
}

# calculate prediction uncertainty based on calibration dataset 
sq_errors_temp <- (subset(temp.cal$temp, temp.cal$year > error.yr1 & temp.cal$year < error.yr2) -
      (subset(temp.rec$temp, temp.rec$year > error.yr1 & temp.rec$year < error.yr2)*sd.prism + mean.prism))^2

# determine graphing limits based on values 
ylims <- c(min(temp.rec$adj - 2*sqrt(mean(sq_errors_temp))), 
        max(temp.rec$adj + 2*sqrt(mean(sq_errors_temp))))

# plot adjusted predictive values 
if (!CRC){
  {plot(temp.rec$year, temp.rec$adj, type="l", ylab="temp", xlab="year", ylim=ylims,main='Anomaly-adjusted Mann temperature reconstruction values')
    
    # plots lines for 95% confidence interval of prediction
    lines(temp.rec$year, temp.rec$adj - 2*sqrt(mean(sq_errors_temp)), col="red", lty=2)
    lines(temp.rec$year, temp.rec$adj + 2*sqrt(mean(sq_errors_temp)), col="red", lty=2)}
}

## Load and prepare ESRL data
## Final is stored in pdsi.cal data frame with year and value 
pdsi.cal <- ncdf4::nc_open(file.path(wd.base,"data/weight","ESRL.data.nc"))
e.lon = ncdf4::ncvar_get(pdsi.cal, 'lon')
e.lat = ncdf4::ncvar_get(pdsi.cal, 'lat')
e.pdsi = ncdf4::ncvar_get(pdsi.cal,'pdsi')

# find closest ESRL grid cell to site coordinates
lon_esrl <- which((e.lon - long_site)^2 == min((e.lon - long_site)^2))
lat_esrl <- which((e.lat - lat_site)^2 == min((e.lat - lat_site)^2))

# convert hours to seconds to put into date format
e.date <- as.POSIXct('1800-01-01 00:00') + as.difftime(ncdf4::ncvar_get(pdsi.cal,'time'), units="hours")
e.year <- year(e.date)

# calculate annual average temperature, ignoring missing values
# annual averages calculated by averaging pdsi values for all months
pdsi.cal <- data.frame(
  pdsi = c(by(e.pdsi[lon_esrl, lat_esrl, ], e.year, mean, na.rm=TRUE)),
  year = unique(e.year))

# plot empirical PDSI values over time
if (!CRC){
  layout(matrix(1:2, 2, 1))
  {plot(pdsi.cal$year, pdsi.cal$pdsi, type='l', 
        main="empirical value in black, reconstruction in red", 
        ylab="PDSI", xlab="year")
    lines(pdsi.rec$year, pdsi.rec$pdsi, 
          col='red')}
}

# calculate squared prediction error in predicting PDSI
pdsi.yr1 = max(min(pdsi.cal$year),min(pdsi.rec$year))-1
pdsi.yr2 = min(max(pdsi.cal$year),max(pdsi.rec$year))+1
sq_errors_pdsi <- (subset(pdsi.cal$pdsi, pdsi.cal$year < pdsi.yr2 & pdsi.cal$year > pdsi.yr1) - 
                     subset(pdsi.rec$pdsi, pdsi.rec$year < pdsi.yr2 & pdsi.rec$year > pdsi.yr1))^2

if (!CRC){
  # plot squared error 
  {plot((pdsi.yr1+1):(pdsi.yr2-1), sq_errors_pdsi, type='l',
        main="prediction error, mean prediction variance in red",
        ylab=TeX("$\\hat{\\sigma}^2$"), xlab="year")
    # variance for the calibration model
    abline(h=mean(sq_errors_pdsi), col="red")}
  
  # plot lbda
  {plot(pdsi.rec$year, pdsi.rec$pdsi,
        type="l", ylim=c(-6, 6), ylab="PDSI", xlab="year",
        main='pdsi calibration dataset')
    
    # plot lower 95% confidence interval
    lines(pdsi.rec$year, pdsi.rec$pdsi-2*sqrt(mean(sq_errors_pdsi)), col='red', lty=2)
    lines(pdsi.rec$year, pdsi.rec$pdsi+2*sqrt(mean(sq_errors_pdsi)), col='red', lty=2)}
}

## Set up two data frames with empirical values (df.cal) and ensemble data (df.ens) for both temperature and PDSI 

# adjust reconstruction data to only include adjusted values 
temp.rec = data.frame(
  year = temp.rec$year,
  temp = temp.rec$adj
)

df.cal <- merge(temp.rec, pdsi.rec, by="year", all=TRUE)
df.ens <- data.frame(
  year  = rep(years, times=n_models),
  temp  = c(temp.ens),
  pdsi  = c(pdsi.ens), 
  model = rep(models, each=length(years))
)

####################
# Step 3: Filtering using T and PDSI 
####################

# Use least squares with constraints inverse problem to determine ensemble 

# save squared error values from prediction found earlier 
s2_temp <- mean(sq_errors_temp)
s2_pdsi <- mean(sq_errors_pdsi)

# find number of years for both calibration and ensemble data 
n_year_cal <- length(df.cal$year)
n_year_ens <- length(unique(df.ens$year))

# find first and last year where data is available for one of the variables and there is ensemble data 
weight.yr1 = min(min(temp.rec$year),min(pdsi.rec$year))
if (weight.yr1 < ens.yr1) weight.yr1 = ens.yr1

weight.yr2 = max(max(temp.rec$year),max(pdsi.rec$year))
if (weight.yr2 > ens.yr2) weight.yr2 = ens.yr2 

# create storage array for weights 
w <- array(0, dim=c(K, length(weight.yr1:weight.yr2), n_models))

# sum to one constraint
E <- matrix(rep(1, n_models), nrow = 1)
F <- 1

# inequality constraints (all weights nonnegative)
G <- diag(1, n_models)
H <- rep(0, n_models)

if (file.exists(file.path(wd.base,"data/weight/PRISM",paste0("filtering-pdsi-",site,"-prism.RData")))) {
  load(file.path(wd.base,"date/weight/PRISM",paste0("filtering-pdsi-",site,"-prism.RData")))
} else {
  for (k in 1:K) {
    
    # draw from distribution for kth filter
    message("Iteration ", k, " out of ", K)
    df.cal$sim_pdsi <- df.cal$pdsi + rnorm(n_year_cal, 0, sqrt(s2_pdsi))
    df.cal$sim_temp <- df.cal$temp + rnorm(n_year_cal, 0, sqrt(s2_temp))
    
    # calculate weights for each year across ensembles
    for (i in weight.yr1:weight.yr2) {
     
      # to track the index value when year does not start at 1
      id = i - weight.yr1 + 1 
      
      # neither variable is available for year --> results in equal weights across ensembles
      if (is.na(subset(df.cal, year == i)$sim_pdsi & is.na(subset(df.cal, year == i)$sim_temp))) {
        w[k, id, ] <- rep(1/n_models, n_models)
        
        # empirical pdsi data is not available for year 
      } else if (is.na(subset(df.cal, year == i)$sim_pdsi)) {
        cal_data <- matrix(subset(df.ens, year == i)$temp, 1, n_models)
        tmp <- try(lsei(A=cal_data, B=subset(df.cal, year == i)$sim_temp, E=E, F=F, G=G, H=H, type = 1)$X)
        if (class(tmp) == "try-error") {
          tmp <<- lsei(A=cal_data, B=subset(df.cal, year == i)$sim_temp, E=E, F=F, G=G, H=H, type = 2)$X
        }
        w[k, id, ] <- tmp
      
        # empirical temperature data is not available for year 
      } else if (is.na(subset(df.cal, year == i)$sim_temp)) {
        cal_data <- matrix(subset(df.ens, year == i)$pdsi, 1, n_models)
        tmp <- try(lsei(A=cal_data, B=subset(df.cal, year == i)$sim_pdsi, E=E, F=F, G=G, H=H, type = 1)$X)
        if (class(tmp) == "try-error") {
          tmp <<- lsei(A=cal_data, B=subset(df.cal, year == i)$sim_pdsi, E=E, F=F, G=G, H=H, type = 2)$X
        }
        w[k, id, ] <- tmp
        
        # if data for both variables are available for year 
      } else {
        cal_data <- t(subset(df.ens, year == i)[, 2:3])
        tmp <- try(lsei(A=cal_data, 
                        B=c(subset(df.cal, year == i)$sim_temp,
                            subset(df.cal, year == i)$sim_pdsi),
                        E=E, F=F, G=G, H=H, type = 1)$X)
        if (class(tmp) == "try-error") {
          tmp <<- lsei(A=cal_data, 
                       B=c(subset(df.cal, year == i)$sim_temp,
                           subset(df.cal, year == i)$sim_pdsi),
                       E=E, F=F, G=G, H=H, type = 2)$X
        }
        
        w[k, id, ] <- tmp
        
      }
    } # end year loop (i)
  } # end iteration loop (k)
  
  # correct to have valid solutions and account for possible computer rounding error
  w[w<0] <- 0 # checking for barely negative weights
  w[w>(1+10e-4)] <- 1 # checking for weights barely over one
  
  # renormalize weights after adjusting for rounding errors
  w_sum <- apply(w, c(1, 2), sum)
  for (k in 1:nrow(w)) {
    for (j in 1:ncol(w)) {
      w[k, j, ] <- w[k, j, ] / w_sum[k, j]
    }
  }
  
  # save Rdata file of all weights for full distribution
  save(w, file=file.path(wd.base,"ensembles",paste0(site,vers),"linkages/weights",paste0("filtering-pdsi-",site,"-prism.RData")))
}

####################
# Step 4: Graphing
####################

# Weights and weighted ensemble data 

# df_weights contains all weights for all k iterations for all ensembles for designated year (tt)
df_weights <- data.frame(w=c(w[, (tt-weight.yr1+1), ]), x=1:K, model=factor(rep(1:n_models, each=K)))


if (plot){
  # g1-g3 plots deal with PDSI values and g4-g6 deal with temperature values 
 
  # g1 shows distribution of ensemble pdsi values and the empirical value as a red line in designated year 
  g1 <- ggplot(data=subset(df.ens, year == tt), aes(x=pdsi)) + 
    geom_histogram(bins=100) + 
    geom_vline(xintercept = subset(df.cal, year == tt)$pdsi, col="red") +
    ggtitle(paste0("ensemble vs. empirical pdsi for year ", tt)) + xlab("pdsi") + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  # g1
  
  # g2 demonstrates the variability in weight values for each ensemble 
  # spikes in this figure indicate large variabiliy in an ensemble's weights
  g2 <- ggplot(data=df_weights, aes(y=w, x=model)) +
    geom_point(aes(color=model)) + theme(legend.position = "none") + 
    ggtitle(paste0("Variability in model weights for year ", tt)) + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  g2
  
  # pdsi_filtered contains the weighted average pdsi across ensembles for each filter for each year 
  pdsi_filtered <- matrix(0, K, length(weight.yr1:weight.yr2))
  for (t in weight.yr1:weight.yr2) {
    cal_data <- subset(df.ens, year == t)$pdsi
    pdsi_filtered[, (t-weight.yr1+1)] <- w[, (t-weight.yr1+1), ] %*% cal_data
  }
  
  # df_filtered contains data on the distribution of the weighted average PDSI across filters
  df_filtered <- data.frame(
    y          = apply(pdsi_filtered, 2, mean),
    lower_50   = apply(pdsi_filtered, 2, quantile, prob=0.25),
    upper_50   = apply(pdsi_filtered, 2, quantile, prob=0.75),
    lower_95   = apply(pdsi_filtered, 2, quantile, prob=0.025),
    upper_95   = apply(pdsi_filtered, 2, quantile, prob=0.975),
    x          = weight.yr1:weight.yr2,
    truth      = subset(df.cal, year >= weight.yr1 & year <= weight.yr2)$pdsi
  )
  
  # g3 plots time series of weighted average pdsi values with quantile information 
  g3 <- ggplot(data=df_filtered, aes(x=x, y=y)) +
    # geom_line(color="red") + 
    geom_line(aes(x=x, y=truth), color="blue") + 
    # geom_line(aes(x=x, y=lbda), color="orange") + 
    geom_ribbon(aes(ymin=lower_50, ymax=upper_50), alpha=0.75, fill="black") +
    geom_ribbon(aes(ymin=lower_95, ymax=upper_95), alpha=0.25, fill="black") + 
    ggtitle("Filtered pdsi 50% CI (dark) and 95%CI (light), empirical pdsi in blue") +
    xlab("time") + ylab("pdsi") + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  g3
  
  # g4 shows distribution of ensemble temperature values and the empirical value as a red line in designated year  
  g4 <- ggplot(data=subset(df.ens, year == tt), aes(x=temp)) + 
    geom_histogram(bins=100) + 
    geom_vline(xintercept = subset(df.cal, year == tt)$temp, col="red") +
    ggtitle(paste0("ensemble vs. empirical temp for year ", tt)) +
    xlab("temp") + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  # g4
  
  # g5 is the same as g2 
  g5 <- ggplot(data=df_weights, aes(y=w, x=model)) +
    geom_point(aes(color=model)) + theme(legend.position = "none") + 
    ggtitle(paste0("Variability in model weights for year ", tt+100)) + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  
  # temp_filtered contains the weighted average temp across ensembles for each filter for each year 
  temp_filtered <- matrix(0, K, length(weight.yr1:weight.yr2))
  for (t in weight.yr1:weight.yr2) {
    cal_data <- subset(df.ens, year == t)$temp
    temp_filtered[, (t-weight.yr1+1)] <- w[, (tt-weight.yr1+1), ] %*% cal_data
  }
  
  # df_filtered contains quantile data on the distribution of the weighted average temperatures across filters
  df_filtered <- data.frame(
    y          = apply(temp_filtered, 2, mean),
    lower_50   = apply(temp_filtered, 2, quantile, prob=0.25),
    upper_50   = apply(temp_filtered, 2, quantile, prob=0.75),
    lower_95   = apply(temp_filtered, 2, quantile, prob=0.025),
    upper_95   = apply(temp_filtered, 2, quantile, prob=0.975),
    x          = weight.yr1:weight.yr2,
    truth      = subset(df.cal, year >= weight.yr1 & year <= weight.yr2)$temp
  )
  
  # g6 plots time series of weighted average temperature values with quantile information 
  g6 <- ggplot(data=df_filtered, aes(x=x, y=y)) +
    # geom_line(color="red") + 
    geom_line(aes(x=x, y=truth), color="blue") + 
    # geom_line(aes(x=x, y=lbda), color="orange") + 
    geom_ribbon(aes(ymin=lower_50, ymax=upper_50), alpha=0.75, fill="black") +
    geom_ribbon(aes(ymin=lower_95, ymax=upper_95), alpha=0.25, fill="black") + 
    ggtitle("Filtered temp 50% CI (dark) and 95%CI (light), empirical temperature in blue") +
    xlab("time") + ylab("temp") + 
    theme(plot.title = element_text(size = 6, face = "bold"))
  # g6
  
  # save all figures to file 
  {png(file=file.path(wd.base,"ensembles",paste0(site,vers),"linkages/weights",
                      paste0("filtering-joint-",site,"-prism.png")),width=8, height=6, 
       units="in", res=400)
    multiplot(g3, g6, g1, g4, g2, g5, cols=2)
    dev.off()}
}


# Visualize weights for 16 random sample years  
# sample from weighting years 
if (plot){
  n = 16 
  if (weight.yr2-weight.yr1+1 < 36 ) n = weight.yr1 - weight.yr2 + 1
  idx <- sample(c(weight.yr1:weight.yr2), n, replace = FALSE)
  
  # graph weights for each of the models for the randomly selected years 
  for (i in 1:n) {
    # find weights for each model 
    df_weights <- data.frame(w=c(w[,(idx[i] - weight.yr1 + 1), ]),
                             x=1:K, 
                             model=factor(rep(1:n_models, each=K)))
    
    assign(paste0("g", i),  ggplot(data=df_weights, aes(y=w, x=model)) +
             geom_point(aes(color=model)) + theme(legend.position = "none") + 
             ggtitle(paste0("Weights for year ", idx[i] )) +
             theme(plot.title = element_text(size = 6, face = "bold")))
  }
  
  # would have to do redo if not 36 possible years to sample, but... 
  # save plots 
  {png(file=file.path(wd.base,"ensembles",paste0(site,vers),"linkages/weights",paste0("model-weights-",site,"-prism.png")))
    multiplot(g1, g2, g3, g4, g5, g6, 
              g7, g8, g9, g10, g11, g12,
              g13, g14, g15, g16, cols=4)
    dev.off()}
  
}
# More figures with weights 

if (!CRC){
  layout(matrix(1:2, 2, 1))
  
  # weighted average pdsi across ensembles for each filter for each year (black)
  # average weighted average pdsi for each year across filters and ensembles (red)
  {{matplot(weight.yr1:weight.yr2, t(pdsi_filtered), type='l', 
            col=adjustcolor("black", alpha.f=0.5), 
            xlab='Year',ylab='PDSI', main='Weighted average PDSI values over time')
    matplot(weight.yr1:weight.yr2,apply(pdsi_filtered, 2, mean), 
            col="red", type='l', add=TRUE)}
    
    # weighted average temp across ensembles for each filter for each year (black)
    # average weighted average temp for each year across filters and ensembles (red)
    {matplot(weight.yr1:weight.yr2, t(temp_filtered), type='l', 
             col=adjustcolor("black", alpha.f=0.5), 
             xlab='Year',ylab='temp', main='Weighted average temp values over time')
      matplot(weight.yr1:weight.yr2,apply(temp_filtered, 2, mean), 
              col="red", type='l', add=TRUE)}
    
    # weights across all models for each filter for given year (tt) 
    matplot(t(w[, (tt-weight.yr1+1), ]), type='l', main="Variability in the weights", 
            xlab="Model ID",ylab='Weight')}
}
