
R version 3.6.2 (2019-12-12) -- "Dark and Stormy Night"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ####################
> # Modeling Met Ensemble Workflow
> # Step 8 :: Generation of ensemble weights 
> ####################
> # This script has been modified so it can be run as a job submission to the CRC's machines. 
> #   
> # Description: This step contains a modified version of John Tipton's filtering script. 
> # The script works with many different data sources, in addition to the met generated by the 
> # first parts of the workflow in order to weight the ensembles based on accuracy in 
> # representing both PDSI and air temperature. These data sources are listed below. 
> 
> # Temparture Data Sources: 
> # reconstruction - MANN data (500-2006)
> # calibration - PRISM data (1895-2017)
> 
> # PDSI Data Sources: 
> # reconstruction - LBDA data (1135-2005)
> # calibration - ESRL data (1850-2014)
> #
> # Required functions: 
> # - multiplot.R
> #
> # Required libraries:
> # - sp
> # - raster
> # - RNetCDF
> # - maps
> # - stringr
> # - lubridate
> # - latex2exp
> # - limSolve
> # - ggplot2
> 
> ###########################################################   
> # ALTER ONLY THESE VARIABLES BEFORE SUBMITTING FOR NEW SITE
> ########################################################### 
> 
> # Load site details
> site = 'GOOSE' # should be identical to paleon site name 
> site.name = 'Goose Egg' # for graph titling purposes
> site.lat  = 43.068496
> site.lon  = -73.297425
> vers=".v2"
> 
> # this should be true if submitting through the CRC so the program is not making unnecessary plots
> CRC = TRUE
> PLOT = TRUE
> 
> # working directory 
> wd.base = '~/met-crc-workflow'
> 
> ########################################################### 
> # Step 1: Set up working directory
> ########################################################### 
> 
> # PRISM dates for anomaly adjustment (based on Mann paper?)
> prism.yr1 = 1960
> prism.yr2 = 1990
> 
> # years for calculation of squared error (?? 1949-1990 right now)
> error.yr1 = 1949
> error.yr2 = 1990
> 
> # year to plot for ensemble weight figures
> plot = TRUE 
> tt = 1750 
> 
> # number of filter draws 
> K = 100 
> 
> # load required libraries 
> # this section is no longer needed because there is a general script to download packages
> #if (!require('sp',lib.loc='~/Rlibs')) install.packages('sp',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('raster',lib.loc='~/Rlibs')) install.packages('raster',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('ncdf4',lib.loc='~/Rlibs')) install.packages('RNetCDF',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('maps',lib.loc='~/Rlibs')) install.packages('maps',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('stringr',lib.loc='~/Rlibs')) install.packages('stringr',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('lubridate',lib.loc='~/Rlibs')) install.packages('lubridate',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('latex2exp',lib.loc='~/Rlibs')) install.packages('latex2exp',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('limSolve',lib.loc='~/Rlibs')) install.packages('limSolve',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('ggplot2',lib.loc='~/Rlibs')) install.packages('ggplot2',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('colorspace',lib.loc='~/Rlibs')) install.packages('colorspace',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('dplyr',lib.loc='~/Rlibs')) install.packages('dplyr',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> #if (!require('reshape2',lib.loc='~/Rlibs')) install.packages('reshape2',lib='~/Rlibs',repos='http://cran.us.r-project.org',dependencies=T)
> 
> require(sp)
Loading required package: sp
Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘sp’
> require(raster,lib.loc='~/Rlibs')
Loading required package: raster
Failed with error:  ‘package ‘sp’ required by ‘raster’ could not be found’
> require(ncdf4,lib.loc='~/Rlibs')
Loading required package: ncdf4
> require(maps,lib.loc='~/Rlibs')
Loading required package: maps
> require(stringr)
Loading required package: stringr
> require(lubridate)
Loading required package: lubridate

Attaching package: ‘lubridate’

The following objects are masked from ‘package:base’:

    date, intersect, setdiff, union

> require(latex2exp,lib.loc='~/Rlibs')
Loading required package: latex2exp
> require(limSolve,lib.loc='~/Rlibs')
Loading required package: limSolve
> require(ggplot2)
Loading required package: ggplot2

Attaching package: ‘ggplot2’

The following object is masked from ‘package:limSolve’:

    resolution

> require(colorspace)
Loading required package: colorspace
> require(reshape2)
Loading required package: reshape2
> 
> # set important paths
> path.func = file.path(wd.base,'functions')
> 
> # source required functions
> source(file.path(path.func,"multiplot.R"))
> 
> # create save directory 
> if (!dir.exists(file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights"))) {
+   dir.create(file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights"),recursive=T)
+ }
> 
> ########################################################### 
> # Step 2: Load and prepare data for filtering
> ########################################################### 
> 
> ## Load air temperature ensemble data
> ## final is temp.ens matrix with rows as years and columns as ensemble members
> tair <- read.csv(file.path(wd.base,"ensembles",paste0(site,vers),'aggregated/month',"Temperature_AllMembers.csv"), 
+                  stringsAsFactors=FALSE, header=TRUE)
> 
> # find the number of ensembles and remove the dates 
> years = rep(0,length(tair$X))
> for (i in 1:length(tair$X)){
+   years[i] = as.integer(strsplit(tair$X[i],'-')[[1]][1])
+ }
> 
> n_models <- ncol(tair) - 1
> ens.yr1 = min(years)
> ens.yr2 = max(years)
> years <- ens.yr1:ens.yr2
> models <- colnames(tair)[-1]
> 
> # convert from monthly means to annual means
> temp.ens <- matrix(0, length(years), n_models)
> for (i in 1:n_models) {
+   temp.ens[, i] <- colMeans(matrix(tair[, i+1], nrow=12))
+ }
> 
> # name columns and rows with ensemble name and year 
> rownames(temp.ens) <- years
> colnames(temp.ens) <- models
> 
> ## Ensemble data
> ## final is in pdsi.ens matrix with rows as years and columns as ensemble members
> pdsi <- read.csv(file.path(wd.base,"ensembles",paste0(site,vers),'aggregated/month',"PDSI_AllMembers.csv"), 
+                  stringsAsFactors=FALSE, header=TRUE)
> 
> # convert from monthly means to annual means
> pdsi.ens <- matrix(0, length(years), n_models)
> for (i in 1:n_models) {
+   # use June, July, and August pdsi for annual summary
+   pdsi.ens[, i] <- colMeans(matrix(pdsi[, i+1], nrow=12)[6:8, ])
+   # pdsi.ens[, i] <- colMeans(matrix(pdsi[, i+1], nrow=12))
+ }
> 
> # name columns and rows with ensemble names and years 
> rownames(pdsi.ens) <- years
> colnames(pdsi.ens) <- colnames(pdsi)[-1]
> 
> # plot ensemble data for annual mean temperature and PDSI
> if (!CRC){
+   layout(matrix(1:2, 2, 1))
+   matplot(years,temp.ens, type='l', col=adjustcolor(1:n_models, alpha.f=0.5),xlab='year',ylab='air temp',main='Ensemble Met Data')
+   matplot(years,pdsi.ens, type='l', col=adjustcolor(1:n_models, alpha.f=0.5),xlab='year',ylab='pdsi')
+ }
> 
> ## MANN data 
> 
> ## Final is stored in temp.rec data frame with year, anomaly, and adjusted values 
> mann.dat <- ncdf4::nc_open(file.path(wd.base,"data/weight","MANN.data.nc"))
> m.lon = ncdf4::ncvar_get(mann.dat,'lon')
> m.lat = ncdf4::ncvar_get(mann.dat,'lat')
> m.tmpanm = ncdf4::ncvar_get(mann.dat,'tmp_anm')
> m.tmp = ncdf4::ncvar_get(mann.dat,'tmp')
> m.time = ncdf4::ncvar_get(mann.dat,'time')
> 
> # find closest point in MANN data to site coordinates
> lon_mann <- which((m.lon - site.lon)^2 == min((m.lon - site.lon)^2))
> lat_mann <- which((m.lat - site.lat)^2 == min((m.lat - site.lat)^2))
> 
> # save site temperature values
> temp.rec = data.frame(
+   temp = m.tmp[lon_mann, lat_mann, ],
+   year = m.time+1901
+ )
> 
> # plot site location on map
> if (!CRC){
+   {image(m.lon, m.lat, m.tmp[, , 1300], 
+          main=paste("spatial reconstruction -", site.name, "given as the green dot"))
+     map(add=TRUE)
+     points(m.lon[lon_mann], m.lat[lat_mann], pch=16, col="green") 
+     
+     # plot reconstructed temperature (MANN data) at site over time
+     plot(m.time + 1901, m.tmpanm[lon_mann, lat_mann, ], 
+          type='l', main=paste("reconstructed temperature at",site.name),
+          ylab="Temperature", xlab="year")
+     lines(m.time + 1901, temp.rec$temp, col="red",
+           type='l')}
+ }
> 
> ## LBDA data
> 
> ## Final is stored in pdsi.rec data frame with year and value
> lbda.dat <- ncdf4::nc_open(file.path(wd.base,"data/weight","LBDA.data.nc")) ## try
> l.lon = ncdf4::ncvar_get(lbda.dat,'lon')
> l.lat = ncdf4::ncvar_get(lbda.dat,'lat')
> l.time = ncdf4::ncvar_get(lbda.dat,'time')
> l.pdsi = ncdf4::ncvar_get(lbda.dat,'pdsi')
> 
> # find LBDA data closest to site coordinates
> lon_lbda <- which((l.lon - site.lon)^2 == min((l.lon - site.lon)^2))
> lat_lbda <- which((l.lat - site.lat)^2 == min((l.lat - site.lat)^2))
> 
> # plot location on map
> if (!CRC){
+   {image(l.lon, l.lat, t(l.pdsi[900, , ]), main=paste("PDSI data -",site.name,'given as green dot'),xlab='longitude',ylab='latitude')
+     map(add=TRUE)
+     points(l.lon[lon_lbda], l.lat[lat_lbda], pch=16, col="green")
+     
+     # plot reconstructed pdsi at site
+     plot(l.time, l.pdsi[, lat_lbda, lon_lbda], 
+          type='l', main=paste("reconstructed PDSI at", site.name), 
+          ylab="PDSI", xlab="year")}
+ }
> 
> # find years with PDSI data at site 
> pdsi.yrs <- which(l.pdsi[,lat_lbda,lon_lbda] != -99.999)
> n_pdsi <- length(pdsi.yrs)
> 
> # save site PDSI
> pdsi.rec = data.frame(
+   pdsi = l.pdsi[pdsi.yrs, lat_lbda, lon_lbda],
+   year = l.time[pdsi.yrs]
+ )
> 
> ## PRISM data 
> 
> ## Final is stored in temp.cal data frame with year and value 
> load(file.path(wd.base,'data/weight/PRISM/paleon_sites',paste0(site,'.meanTemp.Rdata')))
> temp.cal = meanTemp
> year <- substr(rownames(temp.cal), 1, 4)
> 
> # calculate annual average temperature, ignoring missing values
> temp.cal <- data.frame(
+   temp = c(by(apply(temp.cal, 1, mean), year, mean)),
+   year = as.numeric(as.character(unique(year))))
> 
> # convert to degrees F
> temp.cal$temp <- temp.cal$temp * 1.8 + 32
> 
> # adjust temperature calibration dataset, correcting for anomaly using PRISM, save in temp.rec data frame
> sd.prism = sd(subset(temp.cal$temp, temp.cal$year > prism.yr1 & temp.cal$year < prism.yr2))
> mean.prism = mean(subset(temp.cal$temp, temp.cal$year > prism.yr1 & temp.cal$year < prism.yr2))
> temp.rec$adj = temp.rec$temp*sd.prism + mean.prism
> 
> # plot calibration temperature dataset 
> if (!CRC){
+   {plot(temp.cal$year, temp.cal$temp, type='l', ylab="temp", xlab="year", 
+         main="calibration temperature dataset")
+     lines(temp.rec$year, temp.rec$adj, col='red',type='l')
+     
+     # plot temperature anomaly for error years
+     plot(subset(temp.rec$year, temp.rec$year > error.yr1),
+          subset(temp.rec$temp, temp.rec$year > error.yr1),
+          col="red", type='l', ylab="temperature anomaly", xlab="year",
+          main="Mann reconstructed temperature anomaly")}
+ }
> 
> # calculate prediction uncertainty based on calibration dataset 
> sq_errors_temp <- (subset(temp.cal$temp, temp.cal$year > error.yr1 & temp.cal$year < error.yr2) -
+       (subset(temp.rec$temp, temp.rec$year > error.yr1 & temp.rec$year < error.yr2)*sd.prism + mean.prism))^2
> 
> # determine graphing limits based on values 
> ylims <- c(min(temp.rec$adj - 2*sqrt(mean(sq_errors_temp))), 
+         max(temp.rec$adj + 2*sqrt(mean(sq_errors_temp))))
> 
> # plot adjusted predictive values 
> if (!CRC){
+   {plot(temp.rec$year, temp.rec$adj, type="l", ylab="temp", xlab="year", ylim=ylims,main='Anomaly-adjusted Mann temperature reconstruction values')
+     
+     # plots lines for 95% confidence interval of prediction
+     lines(temp.rec$year, temp.rec$adj - 2*sqrt(mean(sq_errors_temp)), col="red", lty=2)
+     lines(temp.rec$year, temp.rec$adj + 2*sqrt(mean(sq_errors_temp)), col="red", lty=2)}
+ }
> 
> ## ESRL data
> 
> ## Final is stored in pdsi.cal data frame with year and value 
> pdsi.cal <- ncdf4::nc_open(file.path(wd.base,"data/weight","ESRL.data.nc"))
> e.lon = ncdf4::ncvar_get(pdsi.cal, 'lon')
> e.lat = ncdf4::ncvar_get(pdsi.cal, 'lat')
> e.pdsi = ncdf4::ncvar_get(pdsi.cal,'pdsi')
> 
> # find closest ESRL grid cell to site coordinates
> lon_esrl <- which((e.lon - site.lon)^2 == min((e.lon - site.lon)^2))
> lat_esrl <- which((e.lat - site.lat)^2 == min((e.lat - site.lat)^2))
> 
> # convert hours to seconds to put into date format
> e.date <- as.POSIXct('1800-01-01 00:00') + as.difftime(ncdf4::ncvar_get(pdsi.cal,'time'), units="hours")
> e.year <- lubridate::year(e.date)
> 
> e.pdsi = data.frame(pdsi =e.pdsi[lon_esrl,lat_esrl,])
> e.pdsi$year = e.year
> 
> # calculate annual average temperature, ignoring missing values
> # annual averages calculated by averaging pdsi values for all months
> pdsi.cal <- data.frame(
+   pdsi.annual_mean = sapply(unique(e.year), function(x){mean(e.pdsi$pdsi[which(e.year==x)][6:8])}), 
+   pdsi.year = unique(e.year)
+ )
> 
> # plot empirical PDSI values over time
> if (!CRC){
+   {plot(pdsi.cal$pdsi.year, pdsi.cal$pdsi.annual_mean, type='l', 
+         main="empirical value in black, reconstruction in red", 
+         ylab="PDSI", xlab="year")
+     lines(pdsi.rec$year, pdsi.rec$pdsi, 
+           col='red')}
+ }
> 
> # calculate squared prediction error in predicting PDSI
> pdsi.yr1 = max(min(pdsi.cal$pdsi.year),min(pdsi.rec$year))-1
> pdsi.yr2 = min(max(pdsi.cal$pdsi.year),max(pdsi.rec$year))+1
> sq_errors_pdsi <- (subset(pdsi.cal$pdsi.annual_mean, pdsi.cal$pdsi.year < pdsi.yr2 & pdsi.cal$pdsi.year > pdsi.yr1) - 
+                      subset(pdsi.rec$pdsi, pdsi.rec$year < pdsi.yr2 & pdsi.rec$year > pdsi.yr1))^2
> 
> if (!CRC){
+   # plot squared error 
+   {plot((pdsi.yr1+1):(pdsi.yr2-1), sq_errors_pdsi, type='l',
+         main="prediction error, mean prediction variance in red",
+         ylab=TeX("$\\hat{\\sigma}^2$"), xlab="year")
+     # variance for the calibration model
+     abline(h=mean(sq_errors_pdsi), col="red")}
+   
+   # plot lbda
+   {plot(pdsi.rec$year, pdsi.rec$pdsi,
+         type="l", ylim=c(-6, 6), ylab="PDSI", xlab="year",
+         main='pdsi calibration dataset')
+     
+     # plot lower 95% confidence interval
+     lines(pdsi.rec$year, pdsi.rec$pdsi-2*sqrt(mean(sq_errors_pdsi)), col='red', lty=2)
+     lines(pdsi.rec$year, pdsi.rec$pdsi+2*sqrt(mean(sq_errors_pdsi)), col='red', lty=2)}
+ }
> 
> ## Set up two data frames with empirical values (df.cal) and ensemble data (df.ens) for both temperature and PDSI 
> # adjust reconstruction data to only include adjusted values 
> temp.rec = data.frame(
+   year = temp.rec$year,
+   temp = temp.rec$adj
+ )
> 
> df.cal <- merge(temp.rec, pdsi.rec, by="year", all=TRUE)
> df.ens <- data.frame(
+   year  = rep(years, times=n_models),
+   temp  = c(temp.ens),
+   pdsi  = c(pdsi.ens), 
+   model = rep(models, each=length(years))
+ )
> 
> ########################################################### 
> # Step 3: Filtering using T and PDSI 
> ########################################################### 
> 
> # Use least squares with constraints inverse problem to determine ensemble 
> 
> # save squared error values from prediction found earlier 
> s2_temp <- mean(sq_errors_temp)
> s2_pdsi <- mean(sq_errors_pdsi)
> 
> # find number of years for both calibration and ensemble data 
> n_year_cal <- length(df.cal$year)
> n_year_ens <- length(unique(df.ens$year))
> 
> # find first and last year where data is available for one of the variables and there is ensemble data 
> weight.yr1 = min(min(temp.rec$year),min(pdsi.rec$year))
> if (weight.yr1 < ens.yr1) weight.yr1 = ens.yr1
> 
> weight.yr2 = max(max(temp.rec$year),max(pdsi.rec$year))
> if (weight.yr2 > ens.yr2) weight.yr2 = ens.yr2 
> 
> # create storage array for weights 
> w <- array(0, dim=c(K, length(weight.yr1:weight.yr2), n_models))
> 
> # sum to one constraint
> E <- matrix(rep(1, n_models), nrow = 1)
> F <- 1
> 
> # inequality constraints (all weights nonnegative)
> G <- diag(1, n_models)
> H <- rep(0, n_models)
> 
> if (file.exists(file.path(wd.base,"ensembles",paste0(site,vers),"completed/weights",paste0("filtering-pdsi-",site,"-prism.RData")))) {
+   print('File already found!')
+   load(file.path(wd.base,"ensembles",paste0(site,vers),"completed/weights",paste0("filtering-pdsi-",site,"-prism.RData")))
+ } else {
+   for (k in 1:K) {
+     
+     # draw from distribution for kth filter
+     message("Iteration ", k, " out of ", K)
+     df.cal$sim_pdsi <- df.cal$pdsi + rnorm(n_year_cal, 0, sqrt(s2_pdsi))
+     df.cal$sim_temp <- df.cal$temp + rnorm(n_year_cal, 0, sqrt(s2_temp))
+     
+     # calculate weights for each year across ensembles
+     for (i in weight.yr1:weight.yr2) {
+      
+       # to track the index value when year does not start at 1
+       id = i - weight.yr1 + 1 
+       
+       # neither variable is available for year --> results in equal weights across ensembles
+       if (is.na(subset(df.cal, year == i)$sim_pdsi & is.na(subset(df.cal, year == i)$sim_temp))) {
+         w[k, id, ] <- rep(1/n_models, n_models)
+         
+         # empirical pdsi data is not available for year 
+       } else if (is.na(subset(df.cal, year == i)$sim_pdsi)) {
+         cal_data <- matrix(subset(df.ens, year == i)$temp, 1, n_models)
+         tmp <- try(lsei(A=cal_data, B=subset(df.cal, year == i)$sim_temp, E=E, F=F, G=G, H=H, type = 1)$X)
+         if (class(tmp) == "try-error") {
+           tmp <<- lsei(A=cal_data, B=subset(df.cal, year == i)$sim_temp, E=E, F=F, G=G, H=H, type = 2)$X
+         }
+         w[k, id, ] <- tmp
+       
+         # empirical temperature data is not available for year 
+       } else if (is.na(subset(df.cal, year == i)$sim_temp)) {
+         cal_data <- matrix(subset(df.ens, year == i)$pdsi, 1, n_models)
+         tmp <- try(lsei(A=cal_data, B=subset(df.cal, year == i)$sim_pdsi, E=E, F=F, G=G, H=H, type = 1)$X)
+         if (class(tmp) == "try-error") {
+           tmp <<- lsei(A=cal_data, B=subset(df.cal, year == i)$sim_pdsi, E=E, F=F, G=G, H=H, type = 2)$X
+         }
+         w[k, id, ] <- tmp
+         
+         # if data for both variables are available for year 
+       } else {
+         cal_data <- t(subset(df.ens, year == i)[, 2:3])
+         tmp <- try(lsei(A=cal_data, 
+                         B=c(subset(df.cal, year == i)$sim_temp,
+                             subset(df.cal, year == i)$sim_pdsi),
+                         E=E, F=F, G=G, H=H, type = 1)$X)
+         if (class(tmp) == "try-error") {
+           tmp <<- lsei(A=cal_data, 
+                        B=c(subset(df.cal, year == i)$sim_temp,
+                            subset(df.cal, year == i)$sim_pdsi),
+                        E=E, F=F, G=G, H=H, type = 2)$X
+         }
+         
+         w[k, id, ] <- tmp
+         
+       }
+     } # end year loop (i)
+   } # end iteration loop (k)
+   
+   # correct to have valid solutions and account for possible computer rounding error
+   w[w<0] <- 0 # checking for barely negative weights
+   w[w>(1+10e-4)] <- 1 # checking for weights barely over one
+   
+   # renormalize weights after adjusting for rounding errors
+   w_sum <- apply(w, c(1, 2), sum)
+   for (k in 1:nrow(w)) {
+     for (j in 1:ncol(w)) {
+       w[k, j, ] <- w[k, j, ] / w_sum[k, j]
+     }
+   }
+   
+   # save Rdata file of all weights for full distribution
+   save(w, file=file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",paste0("filtering-pdsi-",site,"-prism.RData")))
+   
+ }
Iteration 1 out of 100
Iteration 2 out of 100
Iteration 3 out of 100
Iteration 4 out of 100
Iteration 5 out of 100
Iteration 6 out of 100
Iteration 7 out of 100
Iteration 8 out of 100
Iteration 9 out of 100
Iteration 10 out of 100
Iteration 11 out of 100
Iteration 12 out of 100
Iteration 13 out of 100
Iteration 14 out of 100
Iteration 15 out of 100
Iteration 16 out of 100
Iteration 17 out of 100
Iteration 18 out of 100
Iteration 19 out of 100
Iteration 20 out of 100
Iteration 21 out of 100
Iteration 22 out of 100
Iteration 23 out of 100
Iteration 24 out of 100
Iteration 25 out of 100
Iteration 26 out of 100
Iteration 27 out of 100
Iteration 28 out of 100
Iteration 29 out of 100
Iteration 30 out of 100
Iteration 31 out of 100
Iteration 32 out of 100
Iteration 33 out of 100
Iteration 34 out of 100
Iteration 35 out of 100
Iteration 36 out of 100
Iteration 37 out of 100
Iteration 38 out of 100
Iteration 39 out of 100
Iteration 40 out of 100
Iteration 41 out of 100
Iteration 42 out of 100
Iteration 43 out of 100
Iteration 44 out of 100
Iteration 45 out of 100
Iteration 46 out of 100
Iteration 47 out of 100
Iteration 48 out of 100
Iteration 49 out of 100
Iteration 50 out of 100
Iteration 51 out of 100
Iteration 52 out of 100
Iteration 53 out of 100
Iteration 54 out of 100
Iteration 55 out of 100
Iteration 56 out of 100
Iteration 57 out of 100
Iteration 58 out of 100
Iteration 59 out of 100
Iteration 60 out of 100
Iteration 61 out of 100
Iteration 62 out of 100
Iteration 63 out of 100
Iteration 64 out of 100
Iteration 65 out of 100
Iteration 66 out of 100
Iteration 67 out of 100
Iteration 68 out of 100
Iteration 69 out of 100
Iteration 70 out of 100
Iteration 71 out of 100
Iteration 72 out of 100
Iteration 73 out of 100
Iteration 74 out of 100
Iteration 75 out of 100
Iteration 76 out of 100
Iteration 77 out of 100
Iteration 78 out of 100
Iteration 79 out of 100
Iteration 80 out of 100
Iteration 81 out of 100
Iteration 82 out of 100
Iteration 83 out of 100
Iteration 84 out of 100
Iteration 85 out of 100
Iteration 86 out of 100
Iteration 87 out of 100
Iteration 88 out of 100
Iteration 89 out of 100
Iteration 90 out of 100
Iteration 91 out of 100
Iteration 92 out of 100
Iteration 93 out of 100
Iteration 94 out of 100
Iteration 95 out of 100
Iteration 96 out of 100
Iteration 97 out of 100
Iteration 98 out of 100
Iteration 99 out of 100
Iteration 100 out of 100
There were 50 or more warnings (use warnings() to see the first 50)
> 
> ########################################################### 
> # Step 4: Graphing
> ########################################################### 
> 
> ### Graph 1: Ensembles vs. Empirical Values over time ###
> 
> # Weights and weighted ensemble data 
> 
> # df_weights contains all weights for all k iterations for all ensembles for designated year (tt)
> df_weights <- data.frame(w=c(w[, (tt-weight.yr1+1), ]), x=1:K, model=factor(rep(1:n_models, each=K)))
> 
> if (PLOT){
+   
+   # pdsi_filtered contains the weighted average pdsi across ensembles for each filter for each year 
+   pdsi_filtered <- matrix(0, K, length(weight.yr1:weight.yr2))
+   for (t in weight.yr1:weight.yr2) {
+     cal_data <- subset(df.ens, year == t)$pdsi
+     pdsi_filtered[, (t-weight.yr1+1)] <- w[, (t-weight.yr1+1), ] %*% cal_data
+   }
+   
+   # df_filtered contains data on the distribution of the weighted average PDSI across filters
+   df_filtered <- data.frame(
+     y          = apply(pdsi_filtered, 2, mean),
+     lower_50   = apply(pdsi_filtered, 2, quantile, prob=0.25),
+     upper_50   = apply(pdsi_filtered, 2, quantile, prob=0.75),
+     lower_95   = apply(pdsi_filtered, 2, quantile, prob=0.025),
+     upper_95   = apply(pdsi_filtered, 2, quantile, prob=0.975),
+     x          = weight.yr1:weight.yr2,
+     truth      = subset(df.cal, year >= weight.yr1 & year <= weight.yr2)$pdsi
+   )
+ 
+   # g3 plots time series of weighted average pdsi values with quantile information 
+   g3 <- ggplot(data=df_filtered, aes(x=x, y=y)) +
+   #g3 <- ggplot(data=subset(df_filtered, (x >= 1250 & x <= 1350)), aes(x=x,y=y)) + 
+     # geom_line(color="red") + 
+     geom_line(aes(x=x, y=truth), color="blue") + 
+     # geom_line(aes(x=x, y=lbda), color="orange") + 
+     geom_ribbon(aes(ymin=lower_50, ymax=upper_50), alpha=0.75, fill="black") +
+     geom_ribbon(aes(ymin=lower_95, ymax=upper_95), alpha=0.25, fill="black") + 
+     ggtitle("Filtered pdsi 50% CI (dark) and 95%CI (light), empirical pdsi in blue") +
+     xlab("time") + ylab("pdsi") + 
+     theme(plot.title = element_text(size = 6, face = "bold"))
+   
+   
+   # temp_filtered contains the weighted average temp across ensembles for each filter for each year 
+   temp_filtered <- matrix(0, K, length(weight.yr1:weight.yr2))
+   for (t in weight.yr1:weight.yr2) {
+     cal_data <- subset(df.ens, year == t)$temp
+     temp_filtered[, (t-weight.yr1+1)] <- w[, (t-weight.yr1+1), ] %*% cal_data
+   }
+   
+   # df_filtered contains quantile data on the distribution of the weighted average temperatures across filters
+   df_filtered <- data.frame(
+     y          = apply(temp_filtered, 2, mean),
+     lower_50   = apply(temp_filtered, 2, quantile, prob=0.25),
+     upper_50   = apply(temp_filtered, 2, quantile, prob=0.75),
+     lower_95   = apply(temp_filtered, 2, quantile, prob=0.025),
+     upper_95   = apply(temp_filtered, 2, quantile, prob=0.975),
+     x          = weight.yr1:weight.yr2,
+     truth      = subset(df.cal, year >= weight.yr1 & year <= weight.yr2)$temp
+   )
+   
+   # g6 plots time series of weighted average temperature values with quantile information 
+   g6 <- ggplot(data=df_filtered, aes(x=x, y=y)) +
+   #g6 <- ggplot(data=subset(df_filtered, (x >= 1250 & x <= 1350)), aes(x=x,y=y)) + 
+     # geom_line(color="red") + 
+     geom_line(aes(x=x, y=truth), color="blue") + 
+     # geom_line(aes(x=x, y=lbda), color="orange") + 
+     geom_ribbon(aes(ymin=lower_50, ymax=upper_50), alpha=0.75, fill="black") +
+     geom_ribbon(aes(ymin=lower_95, ymax=upper_95), alpha=0.25, fill="black") + 
+     ggtitle("Filtered temp 50% CI (dark) and 95%CI (light), empirical temperature in blue") +
+     xlab("time") + ylab("temp") + 
+     theme(plot.title = element_text(size = 6, face = "bold"))
+   # g6
+   
+   # save all figures to file 
+   {png(file=file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",
+                       paste0("filtering-plot1-",site,".png")),width=8, height=6, 
+        units="in", res=400)
+     multiplot(g3, g6, cols=2)
+     dev.off()}
+ }
Loading required package: grid
null device 
          1 
Warning message:
Removed 1 row(s) containing missing values (geom_path). 
> 
> ### Graph 2: Heat Map of Ensemble Weights over Time ###
> 
> if (PLOT){
+ 
+   # calculate mean weights 
+   w_mean <- apply(w, c(2, 3), mean)
+   dimnames(w_mean) <- list(
+     years  = c(weight.yr1:weight.yr2),
+     models = models)
+   dat <- as.data.frame.table(w_mean, responseName = "weights")
+   dat$years <- as.numeric(as.character(weight.yr1:weight.yr2)) 
+   
+   # plot weights of each ensemble model over time
+   plot2 <-  ggplot(data = dat, aes(years, models, fill = weights)) +
+     geom_tile() +
+     theme_bw() +
+       scale_fill_continuous_sequential(palette = "Heat 2") + 
+     ggtitle("Ensemble Weight 'Heat' Map")+ 
+     theme(axis.text.y = element_text(size = 4))
+   
+   {png(file=file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",
+                       paste0("filtering-plot2-",site,".png")),width=8, height=6, 
+        units="in", res=400)
+   multiplot(plot2,cols=1)
+   dev.off()}
+ }
null device 
          1 
> 
> ### Graph 3: Histogram showing proportion of ensembles that are greater than empirical value over time ###
> 
> if (PLOT){
+   
+   # PDSI 
+   first.pdsi.yr = min(subset(df.cal, !is.na(pdsi))$year)
+   last.pdsi.yr = max(subset(df.cal, !is.na(pdsi))$year)
+   if (first.pdsi.yr > weight.yr1){first = first.pdsi.yr} else{first=weight.yr1}
+   if (last.pdsi.yr < weight.yr2){last = last.pdsi.yr} else{last=weight.yr2}
+   
+   prop = rep(0,length(first:last))
+   ind = 1
+   for (i in first:last){
+     emp = subset(df.cal, year == i)$pdsi
+     vals = df.ens$pdsi[df.ens$year==i]
+     prop[ind] = length(vals[vals >= emp]) / n_models
+     ind = ind + 1
+   }
+   df.plot = data.frame(year = c(first:last), prop = prop)
+   
+   g1 <- ggplot(data = df.plot, aes(x=year, y=prop))+
+     geom_point(aes(x=year, y=prop), color="black") + 
+     ggtitle(paste0("Proportion of over-predicting ensembles for PDSI over time"))+
+     ylim(-.2,1.2) + 
+     theme(plot.title = element_text(size = 10, face = "bold"))+ 
+     geom_hline(yintercept=0.1, col='red')+
+     geom_hline(yintercept=0.9, col='red')+ 
+     stat_smooth(formula=y~x,method='gam')
+   
+   # TEMP 
+   first.temp.yr = min(subset(df.cal, !is.na(temp))$year)
+   last.temp.yr = max(subset(df.cal, !is.na(temp))$year)
+   if (first.temp.yr > weight.yr1){first = first.temp.yr} else{first=weight.yr1}
+   if (last.temp.yr < weight.yr2){last = last.temp.yr} else{last=weight.yr2}
+   
+   prop = rep(0,length(first:last))
+   ind = 1
+   for (i in first:last){
+     emp = subset(df.cal, year == i)$temp
+     vals = df.ens$temp[df.ens$year==i]
+     prop[ind] = length(vals[vals >= emp]) / n_models
+     ind = ind + 1
+   }
+   
+   df.plot = data.frame(year = c(first:last), prop = prop)
+   
+   g2 <- ggplot(data = df.plot, aes(x=year, y=prop))+
+     geom_point(aes(x=year, y=prop),color='black') + 
+     ggtitle(paste0("Proportion of over-predicting ensembles for temperature over time"))+
+     ylim(-.2,1.2) +
+     theme(plot.title = element_text(size = 10, face = "bold"))+ 
+     geom_hline(yintercept=0.1, col='red')+
+     geom_hline(yintercept=0.9, col='red')+ 
+     stat_smooth()
+     
+   
+   {png(file=file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",
+                       paste0("filtering-plot3-",site,".png")),width=8, height=6, 
+        units="in", res=400)
+   multiplot(g1, g2, cols=2)
+   dev.off()}
+ }
`geom_smooth()` using method = 'loess' and formula 'y ~ x'
null device 
          1 
> 
> ########################################################### 
> # Step 5: Conversions to SDA-usable format (average)
> ########################################################### 
> 
> # melt down array for csv file save
> all_weights = melt(w)
> colnames(all_weights) = c('monte_carlo','year', 'climate_model','weights')
> all_weights$year = all_weights$year + weight.yr1 - 1
> all_weights$climate_model = plyr::mapvalues(all_weights$climate_model, 
+                                             from = c(1:length(colnames(pdsi.ens))), 
+                                             to = colnames(pdsi.ens))
> 
> # calculate mean across iterations for each year of each model 
> ensemble_weights_mean <- data.frame (
+   weights               = c(apply(w, c(2, 3), mean)),
+   year                  = weight.yr1:weight.yr2,
+   climate_model         = rep(colnames(pdsi.ens), each=length(weight.yr1:weight.yr2)))
> 
> clim_mods <- colnames(pdsi.ens)
> ens_wts <- numeric(length(clim_mods))
> 
> # calculate mean weight for each model across years 
> for(i in 1:length(clim_mods)){
+   ens_wts[i] <- mean(ensemble_weights_mean[ensemble_weights_mean$climate_model==clim_mods[i],'weights'])
+ }
> 
> ensemble_weights = data.frame(
+   climate_model = clim_mods,
+   wts = ens_wts
+ )
> 
> write.csv(ensemble_weights,
+           file=file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",paste0("ensemble-weights-",site,"-prism.csv")))
> write.csv(ensemble_weights_mean, 
+           file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",paste0("ensemble-weights-monte-carlo-mean-",site,"-prism.csv")))
> write.csv(all_weights, 
+           file.path(wd.base,"ensembles",paste0(site,vers),"completed","weights",paste0("monte-carlo-ensemble-weights-",site,"-prism.csv")))
> 
> 
> proc.time()
    user   system  elapsed 
2988.025   34.341 3030.367 
